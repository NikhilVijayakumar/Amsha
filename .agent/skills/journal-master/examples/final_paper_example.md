# Example: Final Journal Report Output

This is an example of what Journal Master generates after orchestrating all sub-skills.

---

# Amsha: A Clean Architecture Framework for Multi-Agent Orchestration

**Author:** [Your Name]  
**Institution:** [Your University]  
**M.Tech Thesis Project**  
**Date:** February 2026

---

## Abstract

Multi-agent orchestration systems face challenges in maintainability, testability, and architectural clarity. Existing frameworks like LangChain and AutoGPT prioritize rapid prototyping over clean architecture principles, leading to tightly coupled code. This work presents **Amsha**, a novel framework that applies Clean Architecture and repository patterns to agent orchestration. Amsha separates concerns across three layers: Amsha (mathematical core), Bodha (business logic), and Yantra (infrastructure). We implement protocol-based dependency injection and Pydantic-validated immutable models throughout. Experimental results show Amsha maintains ≥90% test coverage while achieving comparable performance to LangChain (234ms avg task execution). The framework has been validated through implementation of 6 production modules totaling 1,505 lines of code.

**Keywords:** Multi-agent systems, Clean Architecture, Repository Pattern, Dependency Injection, Software Engineering

---

## 1. Introduction

### 1.1 Background
[Generated from project context]

### 1.2 Problem Statement
Traditional agent frameworks sacrifice maintainability for convenience, resulting in:
- Tight coupling between business logic and infrastructure
- Difficulty in testing without external dependencies
- Magic numbers and hardcoded configurations
- Unclear separation of concerns

### 1.3 Contributions
This work makes the following contributions:
1. **Clean Architecture for Agents:** First application of hexagonal architecture principles to multi-agent orchestration
2. **Repository Pattern Implementation:** MongoDB-backed repository with protocol-based abstraction
3. **Type-Safe Configuration:** Pydantic models with frozen=True guarantee immutability
4. **Comprehensive Validation:** 90%+ test coverage with unit, integration, and E2E tests

---

## 2. Related Work

[Comparison table generated by visual-generator]

**Table 2.1:** Feature comparison (see Appendix for full table)

Our approach differs by enforcing strict architectural boundaries and eliminating global state.

---

## 3. System Architecture

**Figure 3.1:** Three-tier architecture
[Mermaid diagram from visual-generator showing Amsha/Bodha/Yantra]

The system follows a strict layering principle:
- **Amsha (Core):** Pure functions, no I/O, testable in isolation
- **Bodha (Logic):** Orchestration, depends on Protocols not implementations
- **Yantra (Infrastructure):** All external integrations (DB, LLM, File I/O)

---

## 4. Module Analyses

### 4.1 Crew Forge - Repository Pattern Implementation

#### 4.1.1 Purpose
The Crew Forge module manages agent and task persistence using the Repository pattern.

#### 4.1.2 Mathematical Foundations

**Algorithm 4.1:** Crew creation validation

```latex
Valid(crew) = \begin{cases}
true & \text{if } |agents| \geq 1 \land |tasks| \geq 1 \land \forall t \in tasks: has\_agent(t) \\
false & \text{otherwise}
\end{cases}
```

[Additional LaTeX equations from math-extractor]

#### 4.1.3 Architecture

**Figure 4.1:** Crew Forge class diagram
[Insert class diagram from visual-generator]

**Key Patterns:**
- Repository Pattern for data access abstraction
- Protocol-based DI for testability
- Immutable Pydantic models (frozen=True)

### 4.2 Output Process - Evaluation Pipeline

#### 4.2.1 Weighted Scoring Algorithm

The core evaluation uses a three-stage weighted aggregation:

$$
Score_{final} = \frac{\sum_{i=1}^{3} (w_i \cdot Stage_i)}{\sum_{i=1}^{3} w_i}
$$

[Complete mathematical analysis from math-extractor]

#### 4.2.2 Implementation

**Figure 4.2:** Evaluation sequence diagram
[Insert from visual-generator]

[Continue for modules 4.3 - 4.6...]

---

## 5. Cross-Module Analysis

### 5.1 Module Interactions
**Figure 5.1:** Full system workflow
[Sequence diagram showing how modules cooperate]

### 5.2 Dependency Graph
**Figure 5.2:** Module dependencies
[From cross-module analysis]

Clean architecture is validated: no violations of layer boundaries detected.

---

## 6. Experimental Evaluation

### 6.1 Setup
- **Hardware:** AMD Ryzen 7, 16GB RAM
- **Software:** Python 3.11, Pydantic 2.x, MongoDB 6.0
- **Test Dataset:** 100 synthetic crews with 5 agents and 10 tasks each

### 6.2 Performance Results

**Table 6.1:** Performance benchmarks (from visual-generator)

| Operation | Time (ms) | Memory (MB) |
|:----------|----------:|------------:|
| Crew Creation | 12.3 ± 1.2 | 4.2 |
| Task Execution | 234.7 ± 45.3 | 156.3 |
| Evaluation | 8.1 ± 0.9 | 2.7 |

### 6.3 Code Quality Metrics

**Table 6.2:** Module quality (from visual-generator)

| Module | LOC | Coverage | Grade |
|:-------|----:|:---------|:-----:|
| crew_forge | 450 | 95% | A |
| output_process | 320 | 92% | A |
| [others...] | ... | ... | ... |

---

## 7. Discussion

**Advantages:**
1. Clean separation enables unit testing without MongoDB
2. Protocol-based DI allows swapping implementations
3. Pydantic validation catches errors at boundaries

**Limitations:**
- Additional boilerplate compared to quick-and-dirty approaches
- Requires discipline to maintain boundaries

---

## 8. Future Work

Based on gap analysis (Appendix A):

**Critical:**
1. Add baseline comparisons against LangChain (5h effort)
2. Document all hyperparameters (6h)
3. Ensure reproducibility with random seeds (2h)

**Moderate:**
4. Extend performance profiling (5h)
5. Add literature review section (10h)

Total estimated effort: 28 hours

---

## 9. Conclusion

This work demonstrates that Clean Architecture principles can be successfully applied to multi-agent orchestration without sacrificing performance. The Amsha framework achieves 90%+ test coverage while maintaining competitive execution speeds. The three-tier design (Amsha/Bodha/Yantra) provides clear boundaries that improve maintainability and testability.

---

## References

[1] Martin, R.C. "Clean Architecture", 2017  
[2] CrewAI Documentation, https://docs.crewai.com  
[3] LangChain Documentation, https://python.langchain.com

---

## Appendix A: Gap Analysis

[Full gap report from research-gap-analyst]

**Summary:** 8 gaps identified (3 critical, 3 moderate, 2 minor)

---

## Appendix B: Verification Report

### Automated Checks Performed
✅ All LaTeX equations verified against source code  
✅ All diagrams reflect actual class/module structure  
✅ All performance data traced to measurements  
✅ Variable name consistency checked (math ↔ code)

### Manual Verification Required
⚠️ Baseline comparison results (not yet implemented)  
⚠️ Literature review citations (pending completion)

---

## Appendix C: Source Code Traceability

All claims traced to source:
- Section 4.1: `src/nikhil/amsha/crew_forge/`
- Section 4.2: `src/nikhil/amsha/output_process/`
- Performance data: `crew_monitor` logs

**Repository:** https://github.com/NikhilVijayakumar/Amsha  
**License:** MIT
